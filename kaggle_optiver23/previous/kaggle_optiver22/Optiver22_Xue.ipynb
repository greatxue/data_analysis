{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037413,"end_time":"2022-03-30T12:40:12.174167","exception":false,"start_time":"2022-03-30T12:40:12.136754","status":"completed"},"tags":[]},"source":["# Optiver Realized Volatility Prediction\n","\n","作为全球顶尖的做市商，Optiver 致力于不断改善金融市场，为全球众多交易所的期权、ETF、现金股票、债券和外币创造更好的流动性和价格。 Optiver 的团队花费了无数时间来构建复杂的模型，以预测波动性并不断为最终投资者生成更合适的期权价格。\n","\n","在金融二级市场交易中，波动率是一个非常关键的指标，波动率反映了价格的波动量。对于像 Optiver 这样的做市商来说，准确预测波动性至关重要，期权价格与标的产品的波动性直接相关。\n","\n","\n","在此Kaggle竞赛的前三个月中，参赛者建立模型来预测不同行业数百只股票的短期波动。参赛者拥有数亿行高度精细的市场数据，使用这些数据设计预测 10 分钟周期内的波动率的模型。模型将根据训练后三个月评估期内收集的真实市场数据进行评估。\n","\n",">**提示：**Code 和 Markdown 区域可通过 **Shift + Enter** 快捷键运行。此外，Markdown可以通过双击进入编辑模式。\n","\n","我们将这个notebook分为不同的步骤，你可以使用下面的链接来浏览此notebook。\n","\n","* [Step 1](#step1): 数据概览\n","* [Step 2](#step2): 评价指标\n","* [Step 3](#step3): 探索性数据分析\n","* [Step 4](#step4): 特征工程\n","* [Step 5](#step5): 模型搭建与训练\n","\n","在该项目中包含了如下的问题：\n","\n","* [问题 1](#question1): 根据股票的板块效应，构造相关的特征。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:40:12.251038Z","iopub.status.busy":"2022-03-30T12:40:12.249204Z","iopub.status.idle":"2022-03-30T12:40:16.625554Z","shell.execute_reply":"2022-03-30T12:40:16.624379Z","shell.execute_reply.started":"2022-03-30T10:43:22.142221Z"},"papermill":{"duration":4.416078,"end_time":"2022-03-30T12:40:16.62574","exception":false,"start_time":"2022-03-30T12:40:12.209662","status":"completed"},"tags":[]},"outputs":[],"source":["from IPython.core.display import display, HTML\n","from tqdm.notebook import tqdm\n","\n","import pandas as pd\n","import numpy as np # linear algebra\n","import numpy.matlib\n","from numpy.random import seed\n","import glob\n","import os\n","import gc\n","import copy\n","\n","from joblib import Parallel, delayed\n","\n","from sklearn import preprocessing, model_selection\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import QuantileTransformer\n","from sklearn.metrics import r2_score\n","from sklearn.cluster import KMeans\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.model_selection import GroupKFold,KFold\n","\n","import matplotlib.pyplot as plt \n","import seaborn as sns\n","from scipy.stats import probplot\n","import plotly.express as px\n","\n","import lightgbm as lgb\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader,Dataset\n","\n","path_submissions = '/'\n","\n","target_name = 'target'\n","scores_folds = {}\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# 设置 pandas 的DataFrame可以显示所有的columns\n","pd.set_option('max_columns',None)"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2022-01-20T03:06:43.962831Z","start_time":"2022-01-20T03:06:43.941127Z"},"papermill":{"duration":0.033538,"end_time":"2022-03-30T12:40:16.694019","exception":false,"start_time":"2022-03-30T12:40:16.660481","status":"completed"},"tags":[]},"source":["<a id='step1'></a>\n","# 1. 数据概览\n","\n","除了数据挖掘竞赛中正常提供的训练集`train.csv`外，Optiver还给我们提供了一系列辅助csv数据，分别放在\n","\n","* book_train/test.parquet\n","* trade_train/test.parquet\n","\n","文件夹中。下面我们对这些辅助数据进行简要介绍。\n","\n","## book_train/test.parquet: 订单簿数据\n","\n","订单簿是指按价格排序的特定证券或金融工具的买卖订单的清单。 订单簿列出了在每个价格点的买方数量（bid size）或卖方数量（ask size）。\n","\n","下面是一个股票的订单簿的快照（snapshot）（我们称之为股票 A），如图所示，所有预定的买单都在订单簿的的左侧显示为“bid”，而所有预定的卖单都在订单簿的右侧显示为“ask”（或者offer）\n","\n","![order_book_1](https://www.optiver.com/wp-content/uploads/2021/05/OrderBook3.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:40:16.767497Z","iopub.status.busy":"2022-03-30T12:40:16.766508Z","iopub.status.idle":"2022-03-30T12:40:17.321372Z","shell.execute_reply":"2022-03-30T12:40:17.321868Z","shell.execute_reply.started":"2022-03-30T10:44:06.901927Z"},"papermill":{"duration":0.592854,"end_time":"2022-03-30T12:40:17.322047","exception":false,"start_time":"2022-03-30T12:40:16.729193","status":"completed"},"tags":[]},"outputs":[],"source":["book_example = pd.read_parquet('../input/optiver-realized-volatility-prediction/book_train.parquet/stock_id=0')\n","stock_id = '0'\n","book_example = book_example[book_example['time_id']==5]\n","book_example.loc[:,'stock_id'] = stock_id\n","print('订单簿数据的样例：')\n","book_example.head()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.03557,"end_time":"2022-03-30T12:40:17.393251","exception":false,"start_time":"2022-03-30T12:40:17.357681","status":"completed"},"tags":[]},"source":["\n","## trade_train/test.parquet: 交易数据\n","\n","订单簿是市场交易意图的代表，但是市场需要以相同价格进行交易的买方和卖方。因此，有时当有人想进行股票交易时，他们会查看订单簿并找到有相反交易兴趣的人进行交易。\n","\n","例如，假设当在上一段中的订单簿，我们想购买 20 股 A 股票。然后，我们需要找到一些愿意通过总共卖出 20 股或更多股票来与我们交易的人。我们从最低价开始查看 **ask** 报价：在 148 的水平上有 221 股卖出。我们可以以 148 的价格购买 20 股并确认交易执行。这将是我们交易后产生的股票 A 的订单簿：\n","\n","![order_book2](https://www.optiver.com/wp-content/uploads/2021/05/OrderBook4.png)\n","\n","\n","在这种情况下，卖方卖出 20 股，买方买入 20 股，交易所将匹配买卖双方的订单，并公开广播一条交易消息（比赛数据中的trade）：\n","\n","- 20 股 A 股票在市场上以 148 的价格交易\n","\n","与订单簿数据类似，交易数据对 Optiver 的数据科学家也极为重要，因为它反映了市场的活跃程度。 实际上，金融市场中一些常见的技术信号是直接从交易数据中衍生出来的，例如高低或总成交量。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:40:17.467945Z","iopub.status.busy":"2022-03-30T12:40:17.467315Z","iopub.status.idle":"2022-03-30T12:40:17.536646Z","shell.execute_reply":"2022-03-30T12:40:17.537179Z","shell.execute_reply.started":"2022-03-30T10:44:10.683275Z"},"papermill":{"duration":0.10945,"end_time":"2022-03-30T12:40:17.537353","exception":false,"start_time":"2022-03-30T12:40:17.427903","status":"completed"},"tags":[]},"outputs":[],"source":["trade_example =  pd.read_parquet('../input/optiver-realized-volatility-prediction/trade_train.parquet/stock_id=0')\n","trade_example = trade_example[trade_example['time_id']==5]\n","trade_example.loc[:,'stock_id'] = stock_id\n","print('交易簿数据的样例：')\n","trade_example.head()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037487,"end_time":"2022-03-30T12:40:17.612912","exception":false,"start_time":"2022-03-30T12:40:17.575425","status":"completed"},"tags":[]},"source":["对于交易数据我们有一些补充的介绍，同学可选择性阅读：\n","\n","**做市（Market Making）与市场效率（Market Efficiency）**\n","\n","想象一下，在另一天，股票 A 的订单簿变得低于预期，而我们再次想从所有有意的卖家那里购买 20 股。 如下图所示，这个订单簿没有上一个那么密集，可以说，与上一个订单簿相比，这个订单簿的**流动性较低**。\n","\n","![order_book_3](https://www.optiver.com/wp-content/uploads/2021/05/OrderBook5.png)\n","\n","我们可以在 148 处下单买入。但是，目前没有人愿意在 148 处向我们出售，因此我们的订单将保留在账簿中，等待有人与之交易。 如果我们运气不好，价格会上涨，其他人开始出价 149，我们根本买不到。 或者，我们可以下单以 155 买入。交易所会将此订单与 149 股的未平仓卖单匹配，因此我们以 149 买入 1 手。同样，我们将以 150 的价格买入 12 股 , 151 买入 7 股。与试图以 148 买入相比，没有得到我们想要的交易的风险，但我们最终会以更高的价格买入。\n","\n","我们可以看到，在这样一个低效率的市场中，交易是很困难的，因为交易会更加昂贵，如果我们想要高质量地执行我们的订单，我们需要应对更高的市场风险。 这就是为什么投资者喜欢流动性，无论市场条件多么极端，Optiver 这样的做市商都会提供流动性。\n","\n","做市商是一个公司或个人，他们积极地对证券（股票，期货，期权等）的两侧市场进行报价，同时提供买盘（bid）和卖盘（ask）的市场。 由于做市商会同时给出买入和卖出订单，因此有做市商存在的订单簿将更具流动性，因此将为最终投资者提供更有效的市场，可以自由交易而无需担心订单无法执行。\n","\n","**订单簿统计（Order Book Statistics）**\n","\n","Optiver 数据科学家可以从原始订单簿数据中获得大量统计数据，以反映市场流动性和股票估值。 这些统计数据被证明是任何市场预测算法的基本特征。 下面是一些常见的统计数据， Kaggler 可以从订单簿数据中挖掘更有价值的信号。\n","\n","让我们回到股票 A 的原始订单簿\n","\n","![order_book_1](https://www.optiver.com/wp-content/uploads/2021/05/OrderBook3.png)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037732,"end_time":"2022-03-30T12:40:17.687804","exception":false,"start_time":"2022-03-30T12:40:17.650072","status":"completed"},"tags":[]},"source":["<a id='step2'></a>\n","# 2. 评价指标\n","\n","**买卖差价（bid/ask spread）**\n","\n","由于不同股票在市场上的交易水平不同，我们采用最低要价（ask price）和最高出价（bid price）的比率来计算买卖差价。\n","\n","买卖差价的公式可以写成以下形式：\n","\n","$$BidAskSpread = BestAsk/BestBid -1$$\n","\n","\n","**加权平均价格（weighted averaged price, WAP）**\n","\n","订单簿也是股票估值的主要来源之一。 合理的基于订单簿的估值必须考虑两个因素：订单的水平和规模。 在本次比赛中，使用加权平均价格（WAP）来计算实时股票估值并计算实际波动率作为目标。\n","\n","WAP的公式可以写成如下，它考虑了顶层价格和数量信息：\n","\n","\n","$$ WAP = \\frac{BidPrice_{1}*AskSize_{1} + AskPrice_{1}*BidSize_{1}}{BidSize_{1} + AskSize_{1}} $$\n","\n","\n","可以看到，如果两个订单簿分别在相同的价格水平上同时有出价（bid）和要价（ask），那么有更多要价（ask）的那个订单簿将产生较低的股票估值，因为订单簿中有更多的意向卖家，而更多的卖家意味着市场上**供给更多**导致股票估值较低的事实。\n","\n","请注意，在大多数情况下，在连续交易时间内，订单簿不会出现买盘（bid）高于卖盘（ask）的情况。 换句话说，出价和要价**永远不应该交叉**。\n","\n","在本次比赛中，实际波动率是由 WAP 构建的。 上图的订单簿快照的 WAP 为 147.5317797。\n","\n","\n","**我们如何比较昨天和今天的股票价格？——对数收益率（Log returns）**\n","\n","最简单的方法就是拿差价。这绝对是最直观的方式，但是股票之间的价格差异并不总是具有可比性。例如，假设我们在股票 A 和股票 B 上都投资了 1000 美元，股票 A 从 100 美元涨到 102 美元，股票 B 从 10 美元涨到 11 美元。我们总共有 10 股 A (1000 / 100=10) 产生的利润为 10⋅(102−100)=20，总共 100 股 B 的股票收益为 100 美元。因此，股票 A 的价格涨幅更大，尽管股票 B 的涨幅比例要大得多。\n","\n","我们可以通过将移动除以股票的起始价格来解决上述问题，有效地计算价格变化的百分比，也称为**股票收益率（stock return）**。在我们的例子中，股票 A 的收益率是 $\\frac{102 - 100 }{100} = 2\\%$ ，而股票 B 的收益率是 $\\frac{11 - 10 }{10} = 10\\%$ 。股票收益率与我们投资资本的百分比变化相吻合。\n","\n","收益率在金融中被广泛使用，但是当需要一些数学建模时，首选**对数收益率**。在时间 $t$ 调用 $S_t$股票 $S$ 的价格，我们可以将 $t_1$ 和$t_2$之间的对数回报定义为：\n","\n","$$\n","r_{t_1, t_2} = \\log \\left( \\frac{S_{t_2}}{S_{t_1}} \\right)\n","$$\n","\n","通常，我们查看固定时间间隔内的对数收益率，因此对于 10 分钟的对数收益率，即 $r_t = r_{t - 10 min, t}$。\n","\n","对数收益率具有几个优点，例如：\n","\n","- 它们可以随时间相加 $r_{t_1, t_2} + r_{t_2, t_3} = r_{t_1, t_3}$\n","- 常规收益率不能低于 -100%，而对数收益率没有界限\n","\n","\n","**实际波动率（Realized volatility）**\n","\n","当我们交易期权时，我们模型的一个重要特征是股票对数收益率的标准差。 在长或短的时间间隔内计算的对数收益率的标准差会有所不同，因此通常将其标准化为 1 年期，并且年化的标准差称为波动率。\n","\n","在本次比赛中，我们会获得 10 分钟的账单簿和交易数据，需要预测接下来 10 分钟内的波动率。 波动率将按如下方式计算：\n","\n","我们将计算所有连续订单簿更新的对数收益率，并将 **实际波动率 𝜎（realized volatility）** 定义为对数收益率平方和的平方根。\n","$$\n","\\sigma = \\sqrt{\\sum_{t}r_{t-1, t}^2}\n","$$\n","\n","Optiver使用 **加权平均价格（WAP）** 作为股票价格来计算对数收益率。\n","\n","Optiver希望定义尽可能简单明了， 对没有金融知识的 Kaggler 更友好 。 所以Optiver没有年化波动率，假设对数收益率的平均值为 0。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:40:17.765627Z","iopub.status.busy":"2022-03-30T12:40:17.764893Z","iopub.status.idle":"2022-03-30T12:40:18.201768Z","shell.execute_reply":"2022-03-30T12:40:18.202292Z","shell.execute_reply.started":"2022-03-30T11:11:12.490256Z"},"papermill":{"duration":0.476586,"end_time":"2022-03-30T12:40:18.202461","exception":false,"start_time":"2022-03-30T12:40:17.725875","status":"completed"},"tags":[]},"outputs":[],"source":["book_example['wap'] = (book_example['bid_price1'] * book_example['ask_size1'] +\n","                                book_example['ask_price1'] * book_example['bid_size1']) / (\n","                                       book_example['bid_size1']+ book_example['ask_size1'])\n","print('加权平均价格的计算：')\n","plt.figure(figsize=(20, 8), dpi=100)\n","fig = sns.lineplot(x=book_example['seconds_in_bucket'],y=book_example['wap'])\n","sns.lineplot(x=book_example['seconds_in_bucket'],y=book_example['ask_price1'])\n","sns.lineplot(x=book_example['seconds_in_bucket'],y=book_example['bid_price1'])\n","fig.legend(['wap','ask_price1','bid_price1'])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.042614,"end_time":"2022-03-30T12:40:18.287339","exception":false,"start_time":"2022-03-30T12:40:18.244725","status":"completed"},"tags":[]},"source":["<a id='step3'></a>\n","# 3. 探索性数据分析"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:40:18.37777Z","iopub.status.busy":"2022-03-30T12:40:18.376735Z","iopub.status.idle":"2022-03-30T12:40:19.745059Z","shell.execute_reply":"2022-03-30T12:40:19.745581Z","shell.execute_reply.started":"2022-03-30T11:12:51.725038Z"},"papermill":{"duration":1.416258,"end_time":"2022-03-30T12:40:19.745753","exception":false,"start_time":"2022-03-30T12:40:18.329495","status":"completed"},"tags":[]},"outputs":[],"source":["### 读取数据\n","# Function to read our base train and test set\n","def read_train_test():\n","    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n","    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n","    # Create a key to merge with book and trade data\n","    # 将stock_id和time_id编码为唯一的row_id\n","    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n","    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n","    print('训练集有 {} 只股票'.format(len(train['stock_id'].unique())))\n","    print('训练集有 {} 个时间戳'.format(len(train['time_id'].unique())))\n","    print(f'训练集有 {train.shape[0]} 行')\n","    print(train.head())\n","    return train, test\n","\n","# Read train and test\n","train, test = read_train_test()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.04123,"end_time":"2022-03-30T12:40:19.828116","exception":false,"start_time":"2022-03-30T12:40:19.786886","status":"completed"},"tags":[]},"source":["### 实际波动率的分布情况"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:40:19.918013Z","iopub.status.busy":"2022-03-30T12:40:19.916891Z","iopub.status.idle":"2022-03-30T12:40:23.293191Z","shell.execute_reply":"2022-03-30T12:40:23.292419Z","shell.execute_reply.started":"2022-03-30T11:13:00.128826Z"},"papermill":{"duration":3.421443,"end_time":"2022-03-30T12:40:23.293356","exception":false,"start_time":"2022-03-30T12:40:19.871913","status":"completed"},"tags":[]},"outputs":[],"source":["def visualize_target(target):\n","    \n","    print(f'{target}\\n{\"-\" * len(target)}')\n","        \n","    print(f'Mean: {train[target].mean():.4f}  -  Median: {train[target].median():.4f}  -  Std: {train[target].std():.4f}')\n","    print(f'Min: {train[target].min():.4f}  -  25%: {train[target].quantile(0.25):.4f}  -  50%: {train[target].quantile(0.5):.4f}  -  75%: {train[target].quantile(0.75):.4f}  -  Max: {train[target].max():.4f}')\n","    print(f'Skew: {train[target].skew():.4f}  -  Kurtosis: {train[target].kurtosis():.4f}')\n","    missing_values_count = train[train[target].isnull()].shape[0]\n","    training_samples_count = train.shape[0]\n","    print(f'Missing Values: {missing_values_count}/{training_samples_count} ({missing_values_count * 100 / training_samples_count:.4f}%)')\n","\n","    fig, axes = plt.subplots(ncols=2, figsize=(24, 8), dpi=100)\n","    sns.kdeplot(train[target], label=target, shade =True, ax=axes[0])\n","    axes[0].axvline(train[target].mean(), label=f'{target} Mean', color='r', linewidth=2, linestyle='--')\n","    axes[0].axvline(train[target].median(), label=f'{target} Median', color='b', linewidth=2, linestyle='--')\n","    probplot(train[target], plot=axes[1])\n","    axes[0].legend(prop={'size': 16})\n","    \n","    for i in range(2):\n","        axes[i].tick_params(axis='x', labelsize=12.5, pad=10)\n","        axes[i].tick_params(axis='y', labelsize=12.5, pad=10)\n","        axes[i].set_xlabel('')\n","        axes[i].set_ylabel('')\n","    axes[0].set_title(f'{target} Distribution in Training Set', fontsize=20, pad=15)\n","    axes[1].set_title(f'{target} Probability Plot', fontsize=20, pad=15)\n","\n","    plt.show()\n","    \n","visualize_target('target')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:40:23.39988Z","iopub.status.busy":"2022-03-30T12:40:23.399177Z","iopub.status.idle":"2022-03-30T12:40:25.198791Z","shell.execute_reply":"2022-03-30T12:40:25.199469Z","shell.execute_reply.started":"2022-03-30T11:13:16.834134Z"},"papermill":{"duration":1.859411,"end_time":"2022-03-30T12:40:25.199661","exception":false,"start_time":"2022-03-30T12:40:23.34025","status":"completed"},"tags":[]},"outputs":[],"source":["target_means = train.groupby('stock_id')['target'].mean()\n","target_stds = train.groupby('stock_id')['target'].std()\n","\n","target_means_and_stds = pd.concat([target_means, target_stds], axis=1)\n","target_means_and_stds.columns = ['mean', 'std']\n","target_means_and_stds.sort_values(by='mean', ascending=True, inplace=True)\n","\n","fig, ax = plt.subplots(figsize=(32, 48))\n","ax.barh(\n","    y=np.arange(len(target_means_and_stds)),\n","    width=target_means_and_stds['mean'],\n","    xerr=target_means_and_stds['std'],\n","    align='center',\n","    ecolor='black',\n","    capsize=3\n",")\n","\n","ax.set_yticks(np.arange(len(target_means_and_stds)))\n","ax.set_yticklabels(target_means_and_stds.index)\n","ax.set_xlabel('target', size=20, labelpad=15)\n","ax.set_ylabel('stock_id', size=20, labelpad=15)\n","ax.tick_params(axis='x', labelsize=20, pad=10)\n","ax.tick_params(axis='y', labelsize=20, pad=10)\n","ax.set_title('Mean Realized Volatility of Stocks', size=25, pad=20)\n","\n","plt.show()\n","\n","del target_means, target_stds, target_means_and_stds"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.051558,"end_time":"2022-03-30T12:40:25.302052","exception":false,"start_time":"2022-03-30T12:40:25.250494","status":"completed"},"tags":[]},"source":["我们可以看到，目标变量是高度右偏的，存在一些非常不稳定的股票，它们有异常高的波动率。"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:40:25.418179Z","iopub.status.busy":"2022-03-30T12:40:25.417385Z","iopub.status.idle":"2022-03-30T12:42:26.112703Z","shell.execute_reply":"2022-03-30T12:42:26.113327Z","shell.execute_reply.started":"2022-03-30T11:13:23.755797Z"},"papermill":{"duration":120.756918,"end_time":"2022-03-30T12:42:26.11353","exception":false,"start_time":"2022-03-30T12:40:25.356612","status":"completed"},"tags":[]},"outputs":[],"source":["def read_book_data(dataset, stock_id, sort=False, forward_fill=False):\n","        \n","    book_dtypes = {\n","        'time_id': np.uint16,\n","        'seconds_in_bucket': np.uint16,\n","        'bid_price1': np.float32,\n","        'ask_price1': np.float32,\n","        'bid_price2': np.float32,\n","        'ask_price2': np.float32,\n","        'bid_size1': np.uint32,\n","        'ask_size1': np.uint32,\n","        'bid_size2': np.uint32,\n","        'ask_size2': np.uint32,\n","    }\n","\n","    df_book = pd.read_parquet(f'../input/optiver-realized-volatility-prediction/book_{dataset}.parquet/stock_id={stock_id}')\n","    for column, dtype in book_dtypes.items():\n","        df_book[column] = df_book[column].astype(dtype)\n","    \n","    if sort:\n","        df_book.sort_values(by=['time_id', 'seconds_in_bucket'], inplace=True)\n","        \n","    if forward_fill:\n","        df_book = df_book.set_index(['time_id', 'seconds_in_bucket'])\n","        df_book = df_book.reindex(pd.MultiIndex.from_product([df_book.index.levels[0], np.arange(0, 600)], names=['time_id', 'seconds_in_bucket']), method='ffill')\n","        df_book.reset_index(inplace=True)\n","\n","    return df_book\n","\n","for stock_id in tqdm(sorted(train['stock_id'].unique())):\n","            \n","    df_book = read_book_data('train', stock_id)\n","\n","    # Weighted averaged prices\n","    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] + df_book['ask_price1'] * df_book['bid_size1']) /\\\n","                      (df_book['bid_size1'] + df_book['ask_size1'])\n","    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] + df_book['ask_price2'] * df_book['bid_size2']) /\\\n","                      (df_book['bid_size2'] + df_book['ask_size2'])\n","\n","    # Realized volatilities\n","    for wap in [1, 2]:\n","        df_book[f'log_return_from_wap{wap}'] = np.log(df_book[f'wap{wap}'] / df_book.groupby('time_id')[f'wap{wap}'].shift(1))\n","        df_book[f'squared_log_return_from_wap{wap}'] = df_book[f'log_return_from_wap{wap}'] ** 2\n","        df_book[f'realized_volatility_from_wap{wap}'] = np.sqrt(df_book.groupby('time_id')[f'squared_log_return_from_wap{wap}'].transform('sum'))\n","        df_book.drop(columns=[f'squared_log_return_from_wap{wap}'], inplace=True)            \n","        realized_volatilities = df_book.groupby('time_id')[f'realized_volatility_from_wap{wap}'].first().to_dict()\n","        train.loc[train['stock_id'] == stock_id, f'realized_volatility_from_wap{wap}'] = train[train['stock_id'] == stock_id]['time_id'].map(realized_volatilities)\n","        \n","def visualize_book_time_bucket(stock_id, time_id):\n","    \n","    time_bucket = (train['stock_id'] == stock_id) & (train['time_id'] == time_id)\n","    \n","    target = train.loc[time_bucket, 'target'].iloc[0]\n","    realized_volatility = train.loc[time_bucket, 'realized_volatility_from_wap1'].iloc[0]\n","    df_book = read_book_data('train', stock_id, sort=True, forward_fill=True)\n","    df_book = df_book.set_index('seconds_in_bucket')\n","    \n","    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] + df_book['ask_price1'] * df_book['bid_size1']) /\\\n","                      (df_book['bid_size1'] + df_book['ask_size1'])\n","    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] + df_book['ask_price2'] * df_book['bid_size2']) /\\\n","                      (df_book['bid_size2'] + df_book['ask_size2'])\n","    \n","    fig, axes = plt.subplots(figsize=(32, 30), nrows=2)\n","    \n","    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'bid_price1'], label='bid_price1', lw=2, color='tab:green')\n","    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'ask_price1'], label='ask_price1', lw=2, color='tab:red')\n","    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'bid_price2'], label='bid_price2', alpha=0.3, color='tab:green')\n","    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'ask_price2'], label='ask_price2', alpha=0.3, color='tab:red')\n","    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'wap1'], label='wap1', lw=2, linestyle='--', color='tab:blue')\n","    axes[0].plot(df_book.loc[df_book['time_id'] == time_id, 'wap2'], label='wap2', alpha=0.3, linestyle='--',  color='tab:blue')\n","    \n","    axes[1].plot(df_book.loc[df_book['time_id'] == time_id, 'bid_size1'], label='bid_size1', lw=2, color='tab:green')\n","    axes[1].plot(df_book.loc[df_book['time_id'] == time_id, 'ask_size1'], label='ask_size1', lw=2, color='tab:red')\n","    axes[1].plot(df_book.loc[df_book['time_id'] == time_id, 'bid_size2'], label='bid_size2', alpha=0.3, color='tab:green')\n","    axes[1].plot(df_book.loc[df_book['time_id'] == time_id, 'ask_size2'], label='ask_size2', alpha=0.3, color='tab:red')\n","    \n","    for i in range(2):\n","        axes[i].legend(prop={'size': 18})\n","        axes[i].tick_params(axis='x', labelsize=20, pad=10)\n","        axes[i].tick_params(axis='y', labelsize=20, pad=10)\n","    axes[0].set_ylabel('price', size=20, labelpad=15)\n","    axes[1].set_ylabel('size', size=20, labelpad=15)\n","    \n","    axes[0].set_title(\n","        f'Prices of stock_id {stock_id} time_id {time_id} - Current Realized Volatility: {realized_volatility:.6f} - Next 10 minute Realized Volatility: {target:.6f}',\n","        size=25,\n","        pad=15\n","    )\n","    axes[1].set_title(\n","        f'Sizes of stock_id {stock_id} time_id {time_id} - Current Realized Volatility: {realized_volatility:.6f} - Next 10 minute Realized Volatility: {target:.6f}',\n","        size=25,\n","        pad=15\n","    )\n","    \n","    plt.show()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.052283,"end_time":"2022-03-30T12:42:26.220172","exception":false,"start_time":"2022-03-30T12:42:26.167889","status":"completed"},"tags":[]},"source":["### 使用 visualize_book_time_bucket 函数可视化订单价格和订单大小\n","股票 77 的 24600 时间段是训练集中实际波动率最大的时间段"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:42:26.329302Z","iopub.status.busy":"2022-03-30T12:42:26.328592Z","iopub.status.idle":"2022-03-30T12:42:36.995783Z","shell.execute_reply":"2022-03-30T12:42:36.996357Z","shell.execute_reply.started":"2022-03-30T11:17:25.647363Z"},"papermill":{"duration":10.723128,"end_time":"2022-03-30T12:42:36.996535","exception":false,"start_time":"2022-03-30T12:42:26.273407","status":"completed"},"tags":[]},"outputs":[],"source":["visualize_book_time_bucket(stock_id=77, time_id=24600)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.06765,"end_time":"2022-03-30T12:42:37.1363","exception":false,"start_time":"2022-03-30T12:42:37.06865","status":"completed"},"tags":[]},"source":["<a id='step4'></a>\n","# 4. 特征工程\n","\n","在此竞赛中，我们使用量化特征工程方法进行数据维度的扩充，主要包括：\n","\n","* Weighted averaged price，WAP：加权平均价格\n","* Log return：对数收益率\n","* Realized volatility：历史的波动率\n","* Count unique：表示每个time id中有多少秒有记录，这个特征与标的的流动性相关\n","* Max,Min,Std：抽取最大值、最小值、均值、标准差等特征"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-18T06:59:54.300177Z","start_time":"2022-02-18T06:59:54.255102Z"},"execution":{"iopub.execute_input":"2022-03-30T12:42:37.276071Z","iopub.status.busy":"2022-03-30T12:42:37.274991Z","iopub.status.idle":"2022-03-30T12:42:38.587605Z","shell.execute_reply":"2022-03-30T12:42:38.587089Z","shell.execute_reply.started":"2022-03-30T11:18:06.327788Z"},"papermill":{"duration":1.383346,"end_time":"2022-03-30T12:42:38.587751","exception":false,"start_time":"2022-03-30T12:42:37.204405","status":"completed"},"tags":[]},"outputs":[],"source":["# data directory\n","data_dir = '../input/optiver-realized-volatility-prediction/'\n","train, test = read_train_test()\n","\n","# Function to calculate first WAP\n","def calc_wap1(df):\n","    '''\n","    Weighted averaged price1\n","    bid的量多时，实际价格偏向ask要价\n","    ''' \n","    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n","    return wap\n","\n","# Function to calculate second WAP\n","def calc_wap2(df):\n","    '''\n","    Weighted averaged price\n","    '''\n","    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n","    return wap\n","\n","def calc_wap3(df):\n","    '''\n","    Weighted averaged price\n","    符合一般直觉的加权价格，同一方的size和price相乘\n","    ''' \n","    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n","    return wap\n","\n","def calc_wap4(df):\n","    '''\n","    Weighted averaged price\n","    '''\n","    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n","    return wap\n","\n","# Function to calculate the log of the return\n","# Remember that logb(x / y) = logb(x) - logb(y)\n","def log_return(series):\n","    '''\n","    对series取log之后求差分（得到return的log）\n","    '''\n","    return np.log(series).diff()\n","\n","# Calculate the realized volatility\n","def realized_volatility(series):\n","    '''\n","    对log_return的平方求和后开方得到realized_volatility（series为一定区间内的log_return）\n","    '''\n","    return np.sqrt(np.sum(series**2))\n","\n","# Function to count unique elements of a series\n","def count_unique(series):\n","    '''\n","    用来计算seconds_in_bucket的数量\n","    '''\n","    return len(np.unique(series))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.067419,"end_time":"2022-03-30T12:42:38.724575","exception":false,"start_time":"2022-03-30T12:42:38.657156","status":"completed"},"tags":[]},"source":["## 抽取订单簿数据特征"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:42:38.867701Z","iopub.status.busy":"2022-03-30T12:42:38.866575Z","iopub.status.idle":"2022-03-30T12:42:38.89349Z","shell.execute_reply":"2022-03-30T12:42:38.894039Z","shell.execute_reply.started":"2022-03-30T11:18:10.694164Z"},"papermill":{"duration":0.10042,"end_time":"2022-03-30T12:42:38.894222","exception":false,"start_time":"2022-03-30T12:42:38.793802","status":"completed"},"tags":[]},"outputs":[],"source":["# Function to preprocess book data (for each stock id)\n","def book_preprocessor(file_path):\n","    df = pd.read_parquet(file_path)\n","    # Calculate Wap\n","    # 计算每行的加权平均价格\n","    df['wap1'] = calc_wap1(df)\n","    df['wap2'] = calc_wap2(df)\n","    df['wap3'] = calc_wap3(df)\n","    df['wap4'] = calc_wap4(df)\n","    # Calculate log returns\n","    # 根据加权平均价格计算log_return\n","    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n","    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n","    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n","    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n","    # Calculate wap balance\n","    # 计算bid和ask的差（差值越大，volatility越大）\n","    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n","    # Calculate spread\n","    # 价格差价比率\n","    # 价格差价的差\n","    # volume ask量和bid量的大差值意味着低流动性，会导致高波动率\n","    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n","    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n","    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n","    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n","    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n","    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n","    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n","    \n","    # Dict for aggregations\n","    # 统计特征\n","    create_feature_dict = {\n","        'wap1': [np.sum, np.std],\n","        'wap2': [np.sum, np.std],\n","        'wap3': [np.sum, np.std],\n","        'wap4': [np.sum, np.std],\n","        'log_return1': [realized_volatility],\n","        'log_return2': [realized_volatility],\n","        'log_return3': [realized_volatility],\n","        'log_return4': [realized_volatility],\n","        'wap_balance': [np.sum, np.max],\n","        'price_spread':[np.sum, np.max],\n","        'price_spread2':[np.sum, np.max],\n","        'bid_spread':[np.sum, np.max],\n","        'ask_spread':[np.sum, np.max],\n","        'total_volume':[np.sum, np.max],\n","        'volume_imbalance':[np.sum, np.max],\n","        \"bid_ask_spread\":[np.sum,  np.max],\n","    }\n","    # 波动率特征\n","    create_feature_dict_time = {\n","        'log_return1': [realized_volatility],\n","        'log_return2': [realized_volatility],\n","        'log_return3': [realized_volatility],\n","        'log_return4': [realized_volatility],\n","    }\n","    \n","    # Function to get group stats for different windows (seconds in bucket)\n","    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n","        '''\n","        获取滑动窗口中的特征\n","        '''\n","        # Group by the window\n","        # 根据时间窗口进行分组\n","        # 再对每个time_bucket获取统计特征\n","        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n","        # Rename columns joining suffix\n","        # 对groupby之后产生的多层目录重命名\n","        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n","        # Add a suffix to differentiate windows\n","        # 给列名加后缀 _{seconds_in_bucket} e.g. fea_name_500 \n","        if add_suffix:\n","            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n","        return df_feature\n","    \n","    # Get the stats for different windows\n","    # 获取整个区间的统计特征\n","    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n","    # 获取不同时间窗口的波动率特征 （create_feature_dict_time）\n","    # 500代表在每个time_id区间里seconds_in_bucket>500的记录\n","    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n","    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n","    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n","    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n","    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n","\n","    # Merge all\n","    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n","    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n","    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n","    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n","    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n","    # Drop unnecesary time_ids\n","    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n","    \n","    \n","    # Create row_id so we can merge\n","    stock_id = file_path.split('=')[1]\n","    # row_id 为 stock_id-time_id e.g.\n","    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n","    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n","    return df_feature\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.069637,"end_time":"2022-03-30T12:42:39.032151","exception":false,"start_time":"2022-03-30T12:42:38.962514","status":"completed"},"tags":[]},"source":["## 抽取交易数据特征"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:42:39.178223Z","iopub.status.busy":"2022-03-30T12:42:39.177356Z","iopub.status.idle":"2022-03-30T12:42:39.206586Z","shell.execute_reply":"2022-03-30T12:42:39.207135Z","shell.execute_reply.started":"2022-03-30T11:18:12.125195Z"},"papermill":{"duration":0.105149,"end_time":"2022-03-30T12:42:39.207316","exception":false,"start_time":"2022-03-30T12:42:39.102167","status":"completed"},"tags":[]},"outputs":[],"source":["# Function to preprocess trade data (for each stock id)\n","# 逐个股票的交易数据特征提取\n","def trade_preprocessor(file_path):\n","    df = pd.read_parquet(file_path)\n","    # 获取一个time_bucket中的log_return （对price求差分后取log）\n","    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n","    # 获取成交额\n","    df['amount']=df['price']*df['size']\n","    # Dict for aggregations\n","    # 聚合特征：波动率和统计特征\n","    create_feature_dict = {\n","        'log_return':[realized_volatility],\n","        'seconds_in_bucket':[count_unique],\n","        'size':[np.sum, np.max, np.min],\n","        'order_count':[np.sum,np.max],\n","        'amount':[np.sum,np.max,np.min],\n","    }\n","    # 时间窗口聚合特征\n","    create_feature_dict_time = {\n","        'log_return':[realized_volatility],\n","        'seconds_in_bucket':[count_unique],\n","        'size':[np.sum],\n","        'order_count':[np.sum],\n","    }\n","    # Function to get group stats for different windows (seconds in bucket)\n","    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n","        # Group by the window\n","        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n","        # Rename columns joining suffix\n","        # 对groupby之后产生的多层目录重命名\n","        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n","        # Add a suffix to differentiate windows\n","        if add_suffix:\n","            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n","        return df_feature\n","    \n","\n","    # Get the stats for different windows\n","    # 计算聚合的波动率和统计特征\n","    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n","    # 计算各个时间窗口的特征\n","    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n","    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n","    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n","    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n","    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n","    \n","    def tendency(price, vol):    \n","        # 差分求价格变化的绝对值\n","        df_diff = np.diff(price)\n","        # 计算价格变化的百分比\n","        val = (df_diff/price[1:])*100\n","        # time_bucket内每个时间单位的 变化百分比 x 成交量 求和得到趋势\n","        power = np.sum(val*vol[1:])\n","        return(power)\n","    \n","    lis = []\n","    for n_time_id in df['time_id'].unique():\n","        df_id = df[df['time_id'] == n_time_id]\n","        # 计算成交趋势\n","        tendencyV = tendency(df_id['price'].values, df_id['size'].values) \n","        # 均价以上成交的时间数量（seconds_in_bucket计数）\n","        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n","        # 均价以上成交的时间数量（seconds_in_bucket计数）\n","        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n","        # 价格上涨的次数\n","        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n","        # 价格下跌的次数\n","        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n","        # new\n","        # 价格减均价的绝对值的中位数，衡量价格偏移\n","        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n","        # 价格的平方均值（能量函数）得 time_bucket 的能量\n","        energy = np.mean(df_id['price'].values**2)\n","        # 计算 time_bucket 中价格的四分位数差（75分位数-25分位数）\n","        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n","        \n","        # vol vars\n","        # 成交量减成交量均值的绝对值的中位数，衡量成交量偏移\n","        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))\n","        # 成交量的能量\n","        energy_v = np.sum(df_id['size'].values**2)\n","        # 成交量的四分位数差\n","        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n","        #字典存放到list里\n","        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n","                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n","    \n","    df_lr = pd.DataFrame(lis)\n","        \n","   \n","    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n","    \n","    # Merge all\n","    # 凭借各个时间窗口的特征\n","    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n","    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n","    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n","    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n","    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n","    # Drop unnecesary time_ids\n","    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n","    \n","    \n","    df_feature = df_feature.add_prefix('trade_')\n","    stock_id = file_path.split('=')[1]\n","    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n","    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n","    return df_feature\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.067797,"end_time":"2022-03-30T12:42:39.344855","exception":false,"start_time":"2022-03-30T12:42:39.277058","status":"completed"},"tags":[]},"source":["## 抽取每个时间段、每只股票的统计特征"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:42:39.494357Z","iopub.status.busy":"2022-03-30T12:42:39.493635Z","iopub.status.idle":"2022-03-30T12:42:39.49557Z","shell.execute_reply":"2022-03-30T12:42:39.496118Z","shell.execute_reply.started":"2022-03-30T11:18:14.221128Z"},"papermill":{"duration":0.082866,"end_time":"2022-03-30T12:42:39.496292","exception":false,"start_time":"2022-03-30T12:42:39.413426","status":"completed"},"tags":[]},"outputs":[],"source":["# Function to get group stats for the stock_id and time_id\n","def get_time_stock(df):\n","    '''\n","    获取stock-wise和time_bucket-wise的统计特征\n","    '''\n","    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n","                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n","                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n","\n","\n","    # Group by the stock id\n","    # 获取stock-wise的统计特征\n","    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n","    # Rename columns joining suffix\n","    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n","    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n","\n","    # Group by the stock id\n","    # 获取time_bucket-wise的统计特征\n","    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n","    # Rename columns joining suffix\n","    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n","    df_time_id = df_time_id.add_suffix('_' + 'time')\n","    \n","    # Merge with original dataframe\n","    # 和训练集或测试集merge\n","    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n","    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n","    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n","    return df\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.068484,"end_time":"2022-03-30T12:42:39.631108","exception":false,"start_time":"2022-03-30T12:42:39.562624","status":"completed"},"tags":[]},"source":["## 并行化，缩短运行时间"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T12:42:39.771898Z","iopub.status.busy":"2022-03-30T12:42:39.771192Z","iopub.status.idle":"2022-03-30T12:42:39.779127Z","shell.execute_reply":"2022-03-30T12:42:39.779637Z","shell.execute_reply.started":"2022-03-30T11:18:16.280375Z"},"papermill":{"duration":0.079524,"end_time":"2022-03-30T12:42:39.77981","exception":false,"start_time":"2022-03-30T12:42:39.700286","status":"completed"},"tags":[]},"outputs":[],"source":["# Funtion to make preprocessing function in parallel (for each stock id)\n","def preprocessor(list_stock_ids, is_train = True):\n","    \n","    # Parrallel for loop\n","    # 对每个stock并行处理函数\n","    def for_joblib(stock_id):\n","        # Train\n","        if is_train:\n","            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n","            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n","        # Test\n","        else:\n","            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n","            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n","    \n","        # Preprocess book and trade data and merge them\n","        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n","        \n","        # Return the merge dataframe\n","        return df_tmp\n","    \n","    # Use parallel api to call paralle for loop\n","    # 使用 delayed 和 Parallel 并行计算特征（通常与pd.DataFrame的apply函数配合使用）\n","    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n","    # Concatenate all the dataframes that return from Parallel\n","    df = pd.concat(df, ignore_index = True)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-18T07:42:37.707916Z","start_time":"2022-02-18T06:59:54.302975Z"},"execution":{"iopub.execute_input":"2022-03-30T12:42:39.918413Z","iopub.status.busy":"2022-03-30T12:42:39.917679Z","iopub.status.idle":"2022-03-30T13:17:38.294349Z","shell.execute_reply":"2022-03-30T13:17:38.293711Z","shell.execute_reply.started":"2022-03-30T11:18:18.534742Z"},"papermill":{"duration":2098.448064,"end_time":"2022-03-30T13:17:38.294516","exception":false,"start_time":"2022-03-30T12:42:39.846452","status":"completed"},"tags":[]},"outputs":[],"source":["# Get unique stock ids \n","train_stock_ids = train['stock_id'].unique()\n","# Preprocess them using Parallel and our single stock id functions\n","train_ = preprocessor(train_stock_ids, is_train = True)\n","train = train.merge(train_, on = ['row_id'], how = 'left')\n","\n","# Get unique stock ids \n","test_stock_ids = test['stock_id'].unique()\n","# Preprocess them using Parallel and our single stock id functions\n","test_ = preprocessor(test_stock_ids, is_train = False)\n","test = test.merge(test_, on = ['row_id'], how = 'left')\n","\n","# Get group stats of time_id and stock_id\n","train = get_time_stock(train)\n","test = get_time_stock(test)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.136968,"end_time":"2022-03-30T13:17:38.604536","exception":false,"start_time":"2022-03-30T13:17:38.467568","status":"completed"},"tags":[]},"source":["### 还可以增加一些 tau 特征"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-18T07:42:38.398488Z","start_time":"2022-02-18T07:42:37.711769Z"},"execution":{"iopub.execute_input":"2022-03-30T13:17:38.820906Z","iopub.status.busy":"2022-03-30T13:17:38.819788Z","iopub.status.idle":"2022-03-30T13:17:38.946333Z","shell.execute_reply":"2022-03-30T13:17:38.944139Z","shell.execute_reply.started":"2022-03-30T11:58:00.703554Z"},"papermill":{"duration":0.239692,"end_time":"2022-03-30T13:17:38.946504","exception":false,"start_time":"2022-03-30T13:17:38.706812","status":"completed"},"tags":[]},"outputs":[],"source":["# 对seconds_in_bucket_count和order_count构造tau特征\n","# 因为这是对特征的单调变换，对GBDT模型（LightGBM,XGBoost）影响很小，对神经网络可能有影响\n","train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n","test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n","train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n","test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n","train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n","test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n","train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n","test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n","\n","train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n","test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n","train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n","test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n","train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n","test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n","train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n","test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n","\n","# 对tau特征再做差\n","train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n","test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']\n","\n","# 我们现在有194个特征了\n","colNames = [col for col in list(train.columns)\n","            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n","len(colNames)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.164538,"end_time":"2022-03-30T13:17:39.282035","exception":false,"start_time":"2022-03-30T13:17:39.117497","status":"completed"},"tags":[]},"source":["<a id='question1'></a>\n","## 问题 1：\n","\n","根据股票的板块效应（**相同板块的股票，走势应该相似，而不同板块的股票，走势差异较大**），构造相关的特征（**提示：使用相关系数，聚类算法，再计算每个cluster的统计特征**）"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-30T13:17:39.579059Z","iopub.status.busy":"2022-03-30T13:17:39.57102Z","iopub.status.idle":"2022-03-30T13:17:40.034525Z","shell.execute_reply":"2022-03-30T13:17:40.033618Z","shell.execute_reply.started":"2022-03-30T11:58:04.567037Z"},"papermill":{"duration":0.65138,"end_time":"2022-03-30T13:17:40.034717","exception":false,"start_time":"2022-03-30T13:17:39.383337","status":"completed"},"tags":[]},"outputs":[],"source":["train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n","train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n","\n","################在下方构造特征####################\n","##############计算皮尔森相关系数##################\n","\n","\n","##################K-Means聚类#####################\n","\n","\n","#####从训练集、测试集中抽取每个cluster的数据######\n","\n","\n","#############计算每个cluster的特征################\n","\n","\n","###########新特征与训练集，测试集拼接#############\n","\n","\n","################这是一大段代码####################"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-18T07:42:56.766611Z","start_time":"2022-02-18T07:42:55.712089Z"},"execution":{"iopub.execute_input":"2022-03-30T13:17:40.230855Z","iopub.status.busy":"2022-03-30T13:17:40.230051Z","iopub.status.idle":"2022-03-30T13:19:09.248364Z","shell.execute_reply":"2022-03-30T13:19:09.248957Z","shell.execute_reply.started":"2022-03-30T11:58:05.802808Z"},"papermill":{"duration":89.123508,"end_time":"2022-03-30T13:19:09.249378","exception":false,"start_time":"2022-03-30T13:17:40.12587","status":"completed"},"tags":[]},"outputs":[],"source":["# stock_id作为列名，time_id作为index，value为log_return1_realized_volatility的dataframe\n","train_p2 = train.pivot(index='time_id', columns='stock_id', values='log_return1_realized_volatility')\n","# 每列的均值填充缺失值\n","train_p2 = train_p2.fillna(train_p2.mean())\n","\n","# 对测试集相同操作\n","test_p2 = test.pivot(index='time_id', columns='stock_id', values='log_return1_realized_volatility')\n","# 测试集的time_id少于20个的情况下\n","if len(test_p2)<=20:\n","    test_p2[0] =  [0.001,0.222,0.333]\n","    # 用训练集来测试代码\n","    # 测试集的time_id和训练集不一致\n","    test_p2 = pd.concat([train_p2.head(20),test_p2])\n","# 用训练集均值填充测试集\n","test_p2 = test_p2.fillna(test_p2.mean())\n",",\n","from sklearn.neighbors import NearestNeighbors\n","# 用 (time_id，log_return1_realized_volatility) 做特征，用KNN\n","neigh = NearestNeighbors(n_neighbors=10)\n","nbrs = neigh.fit(train_p2)\n","# 用KNN获得每个time_id最近（用112个stock的log_return1_realized_volatility计算距离）9个time_id的Index\n","neighb = nbrs.kneighbors(train_p2,  return_distance=False)\n","neighb_dict = {}\n","# 将每个time_id最近的9个time_id放到字典中\n","# {time_id:[nb1,nb2,nb3...,nb9],}\n","for i,x in enumerate(train_p2.index):\n","    sss = train_p2.index\n","    neighb_dict[sss[neighb[i][0]]] = [sss[neighb[i][1]],sss[neighb[i][2]], sss[neighb[i][3]], sss[neighb[i][4]]]\n","    \n","# 筛选特征,下列步骤为扩充特征，将距离最近的两个time_id的部分特征的均值加入到特征中\n","timecols = ['log_return1_realized_volatility','total_volume_sum',\\\n","           'trade_size_sum','trade_order_count_sum','price_spread_sum',\\\n","           'bid_spread_sum','ask_spread_sum']\n","\n","train2 = []\n","# 遍历train中的所有time_id\n","for xx in tqdm(train.groupby('time_id')):\n","    # xx[0]为当前遍历的time_id,获取train中离xx[0]最近的2个time_id的数据的df\n","    tmp = train[train.time_id.isin(neighb_dict[xx[0]][:2])]\n","    # tmp2为xx[0]的数据的df\n","    tmp2 = xx[1]\n","    # tmp3为xx[0]最近的两个time_id的timecols特征的均值\n","    tmp3 = tmp.groupby('stock_id')[timecols].mean()\n","    # 重命名列名\n","    tmp3.columns = [str(xx)+'_c2' for xx in tmp3.columns]\n","    tmp3 = tmp3.reset_index()\n","    # 与xx[0]的df拼接，扩充特征\n","    tmp2 = pd.merge(tmp2,tmp3,how='left',on='stock_id')\n","    train2.append(tmp2)\n","# 将所有数据拼接\n","train2 = pd.concat(train2).reset_index(drop=True)\n","\n","# 对于测试集，重新计算time_id之间的距离\n","from sklearn.neighbors import NearestNeighbors\n","neigh = NearestNeighbors(n_neighbors=10)\n","nbrs = neigh.fit(test_p2)\n","neighb = nbrs.kneighbors(test_p2,  return_distance=False)\n","neighb_dict_test = {}\n","for i,x in enumerate(test_p2.index):\n","    sss = test_p2.index\n","    neighb_dict_test[sss[neighb[i][0]]] = [sss[neighb[i][1]],sss[neighb[i][2]], sss[neighb[i][3]], sss[neighb[i][4]]]\n","    \n","test2 = []\n","for xx in tqdm(test.groupby('time_id')):\n","    tmp = test[test.time_id.isin(neighb_dict_test[xx[0]][:2])]\n","    tmp2 = xx[1]\n","    tmp3 = tmp.groupby('stock_id')[timecols].mean()\n","    tmp3.columns = [str(xx)+'_c2' for xx in tmp3.columns]\n","    tmp3 = tmp3.reset_index()\n","    tmp2 = pd.merge(tmp2,tmp3,how='left',on='stock_id')\n","    test2.append(tmp2)\n","test2 = pd.concat(test2).reset_index(drop=True)\n","\n","train = train2.copy()\n","test = test2.copy()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.071451,"end_time":"2022-03-30T13:19:09.395659","exception":false,"start_time":"2022-03-30T13:19:09.324208","status":"completed"},"tags":[]},"source":["<a id='step5'></a>\n","# 5. 模型搭建与训练"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2022-02-16T08:43:11.984248Z","start_time":"2022-02-16T08:33:31.834004Z"},"execution":{"iopub.execute_input":"2022-03-30T13:19:09.542542Z","iopub.status.busy":"2022-03-30T13:19:09.541436Z","iopub.status.idle":"2022-03-30T13:24:10.063553Z","shell.execute_reply":"2022-03-30T13:24:10.06279Z"},"papermill":{"duration":300.596468,"end_time":"2022-03-30T13:24:10.063768","exception":false,"start_time":"2022-03-30T13:19:09.4673","status":"completed"},"tags":[]},"outputs":[],"source":["# 选用LightGBM模型\n","seed0=2021\n","params = {\n","    'objective': 'rmse',\n","    'boosting_type': 'gbdt',\n","    'max_depth': -1,\n","    'max_bin':100,\n","    'min_data_in_leaf':500,\n","    'learning_rate': 0.03,\n","    'subsample': 0.72,\n","    'subsample_freq': 4,\n","    'feature_fraction': 0.5,\n","    'lambda_l1': 0.5,\n","    'lambda_l2': 1.0,\n","    'categorical_column':[0],\n","    'seed':seed0,\n","    'feature_fraction_seed': seed0,\n","    'bagging_seed': seed0,\n","    'drop_seed': seed0,\n","    'data_random_seed': seed0,\n","    'n_jobs':-1,\n","    'verbose': -1}\n","\n","# Function to early stop with root mean squared percentage error\n","def rmspe(y_true, y_pred):\n","    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n","\n","def feval_rmspe(y_pred, lgb_train):\n","    y_true = lgb_train.get_label()\n","    return 'RMSPE', rmspe(y_true, y_pred), False\n","\n","def train_and_evaluate_lgb(train, test, params):\n","    # Hyperparammeters (just basic)\n","    \n","    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n","    y = train['target']\n","    # Create out of folds array\n","    oof_predictions = np.zeros(train.shape[0])\n","    # Create test array to store predictions\n","    test_predictions = np.zeros(test.shape[0])\n","    # Create a KFold object\n","    kfoldgroup = GroupKFold(n_splits = 5)\n","    for fold, (trn_ind, val_ind) in enumerate(kfoldgroup.split(range(len(train)),train.target,train.time_id)):\n","\n","        print(f'Training fold {fold + 1}')\n","        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n","        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n","        # Root mean squared percentage error weights\n","        train_weights = 1 / np.square(y_train)\n","        val_weights = 1 / np.square(y_val)\n","        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n","        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n","        model = lgb.train(params = params,\n","                          num_boost_round=1000,\n","                          train_set = train_dataset, \n","                          valid_sets = [train_dataset, val_dataset], \n","                          verbose_eval = 250,\n","                          early_stopping_rounds=50,\n","                          feval = feval_rmspe)\n","        # Add predictions to the out of folds array\n","        oof_predictions[val_ind] = model.predict(x_val[features])\n","        # Predict the test set\n","        test_predictions += model.predict(test[features]) / 5\n","    rmspe_score = rmspe(y, oof_predictions)\n","    print(f'Our out of folds RMSPE is {rmspe_score}')\n","    lgb.plot_importance(model,max_num_features=20)\n","    # Return test predictions\n","    return test_predictions\n","\n","# Traing and evaluate\n","predictions_lgb= train_and_evaluate_lgb(train, test, params)\n","test['target'] = predictions_lgb\n","test[['row_id', 'target']].to_csv('submission.csv',index = False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":2344753,"sourceId":27233,"sourceType":"competition"}],"dockerImageVersionId":30207,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
